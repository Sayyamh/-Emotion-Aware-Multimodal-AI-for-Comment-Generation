{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ðŸ”¹ Set Your Hugging Face Token Here\n",
    "HUGGINGFACE_TOKEN = \"\"  # Replace with your actual token\n",
    "\n",
    "# ðŸ”¹ Load Mistral model (Make sure you have access!)\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "sentiment_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    use_auth_token=HUGGINGFACE_TOKEN  # âœ… Correct way to authenticate\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_auth_token=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from transformers import (\n",
    "    BlipProcessor, BlipForConditionalGeneration,\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    CLIPProcessor, CLIPModel\n",
    ")\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Emotion labels\n",
    "emotion_labels = [\n",
    "    \"happy\", \"sad\", \"angry\", \"surprised\", \"neutral\",\n",
    "    \"fearful\", \"disgusted\", \"excited\", \"peaceful\"\n",
    "]\n",
    "\n",
    "# Load models\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", trust_remote_code=True)\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Emotion Detection\n",
    "def detect_emotion(image: Image.Image):\n",
    "    inputs = clip_processor(\n",
    "        text=[f\"This image makes me feel {emotion}.\" for emotion in emotion_labels],\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits = outputs.logits_per_image\n",
    "        probs = logits.softmax(dim=1)\n",
    "\n",
    "    best_emotion = emotion_labels[probs.argmax()]\n",
    "    confidence = probs[0][probs.argmax()].item()\n",
    "    return best_emotion, confidence\n",
    "\n",
    "# Generate Comment\n",
    "def generate_comment(image: Image.Image, emotion_label: str, mode: str = \"short\"):\n",
    "    blip_inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    caption_ids = blip_model.generate(**blip_inputs)\n",
    "    caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    if mode == \"short\":\n",
    "        prompt = f\"\"\"<s>[INST] Here's a description of an image: \"{caption}\". The person in the image is feeling \"{emotion_label}\". Write a *one-line*, expressive, emotional comment that reflects this vibe. Be human-like, creative, and fun. [/INST]\"\"\"\n",
    "        max_tokens = 40\n",
    "    else:\n",
    "        prompt = f\"\"\"<s>[INST] Here's a description of an image: \"{caption}\". The person in the image is feeling \"{emotion_label}\". Write a short *paragraph* that's expressive, vivid, and emotional. Capture the atmosphere and the feeling. Be poetic and engaging. [/INST]\"\"\"\n",
    "        max_tokens = 100\n",
    "\n",
    "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\").to(mistral_model.device)\n",
    "    outputs = mistral_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.9,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    response = mistral_tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    comment = response.split('[/INST]')[-1].strip()\n",
    "\n",
    "    # Estimate Perplexity\n",
    "    probs = torch.stack(outputs.scores).softmax(dim=-1)\n",
    "    gen_ids = outputs.sequences[0][inputs.input_ids.shape[1]:]\n",
    "    gen_probs = [probs[i][0][token.item()].item() for i, token in enumerate(gen_ids)]\n",
    "    perplexity = torch.exp(-torch.log(torch.tensor(gen_probs)).mean()).item()\n",
    "\n",
    "    return caption, comment, perplexity\n",
    "\n",
    "# Sentiment\n",
    "def evaluate_comment(comment):\n",
    "    sentiment = analyzer.polarity_scores(comment)\n",
    "    valence = sentiment[\"compound\"]\n",
    "    sentiment_type = (\n",
    "        \"positive\" if valence > 0.05 else\n",
    "        \"negative\" if valence < -0.05 else\n",
    "        \"neutral\"\n",
    "    )\n",
    "    return sentiment_type, valence\n",
    "\n",
    "# Main Gradio pipeline\n",
    "def process(image, mode, emotion_choice):\n",
    "    detected_emotion, confidence = detect_emotion(image)\n",
    "    emotion_used = emotion_choice if emotion_choice != \"auto\" else detected_emotion\n",
    "    caption, comment, perplexity = generate_comment(image, emotion_used, mode)\n",
    "    sentiment_type, valence = evaluate_comment(comment)\n",
    "\n",
    "    return (\n",
    "        f\"ðŸ–¼ Caption: {caption}\",\n",
    "        f\"ðŸŽ­ Detected Emotion: {detected_emotion} (confidence: {confidence:.2f})\",\n",
    "        f\"ðŸ§  Emotion Used: {emotion_used}\",\n",
    "        f\"ðŸ’¬ Emotional Comment:\\n> {comment}\",\n",
    "        f\"ðŸ“Š Sentiment: {sentiment_type} (valence: {valence:.2f})\",\n",
    "        f\"ðŸ“‰ Approx. Perplexity: {perplexity:.2f}\"\n",
    "    )\n",
    "\n",
    "# Gradio Interface\n",
    "demo = gr.Interface(\n",
    "    fn=process,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload an Image\"),\n",
    "        gr.Radio(choices=[\"short\", \"paragraph\"], value=\"short\", label=\"Comment Mode\"),\n",
    "        gr.Dropdown([\"auto\"] + emotion_labels, value=\"auto\", label=\"Emotion (or auto-detect)\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Image Caption\"),\n",
    "        gr.Textbox(label=\"Detected Emotion\"),\n",
    "        gr.Textbox(label=\"Emotion Used\"),\n",
    "        gr.Textbox(label=\"Generated Comment\"),\n",
    "        gr.Textbox(label=\"Sentiment\"),\n",
    "        gr.Textbox(label=\"Perplexity\"),\n",
    "    ],\n",
    "    title=\"ðŸ§  Emotional Comment Generator\",\n",
    "    description=\"Upload an image, and this app generates an emotional comment using Mistral + BLIP + CLIP. Detects emotion, sentiment, and perplexity!\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
